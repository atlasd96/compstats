---
title: "Atlas-PS 10"
author: "David Atlas"
date: "10/31/2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem 1
We define an orthogonal function family to be a set of functions $F$ for which $<f, g> = 0 \forall f \neq g, f \in F, g \in F$. Here, 
we define the inner product of two functions with respect to a weight function $w(x)$ to be $\int_D f(x)g(x)w(x)dx$. In the given problem,  $w(x) = 1$, so we 
needn't worry about it.

We start by asserting 3 commonly known identities.
\begin{align}
sin(x)sin(y) &= \frac{1}{2}(cos(x-y) - cos(x + y)) \\
cos(x)cos(y) &= \frac{1}{2}(cos(x+y) + cos(x - y)) \\
sin(x)cos(x) &= \frac{1}{2}(sin(x+y) + sin(x-y))
\end{align}

As both $\int_0^{2\pi}cos(kx) dx = 0$ and $\int_0^{2\pi}sin(kx) dx = 0$ are trivial for all $k \in \mathbb{N}$, we won't discuss the inner product of 1 and any of the functions in the family.

Next, we invoke the identity to say the following: 

Let $k, j$ be natural numbers. Without loss of generalization, we can say that  $k < j$.
\begin{align*}
\int_0^{2\pi}cos(jx)cos(kx)dx &= 
\frac{1}{2} \int_0^{2 \pi} cos((j + k) x) + cos((j - k )x) \\
\frac{1}{2} [\frac{sin((j+k)x)}{j+k} + \frac{sin((j - k)x)}{j-k}\vert_0^{2\pi}.
\end{align*}

Note that $\int_0^{2\pi}sin(kx) dx = 0$ , implying that $\int_0^{2\pi}cos(jx)cos(kx)dx = 0 \forall j, k \in \mathbb{N}, j\neq k$.

Let $k, j$ be natural numbers. Without loss of generalization, we can say that  $k < j$.
\begin{align*}
\int_0^{2\pi}sin(jx)sin(kx)dx &= 
\frac{1}{2} \int_0^{2 \pi} cos((j - k) x) - cos((j + k )x) \\
\frac{1}{2} [\frac{sin((j-k)x)}{j-k} - \frac{sin((j + k)x)}{j+k}] \vert_0^{2\pi}.
\end{align*}
Again, both terms are equal to zero for any natural numbers $j$ and $k$ that aren't equal to each other. 

Let $k, j$ be natural numbers. Without loss of generalization, we can say that  $k < j$$.
\begin{align*}
\int_0^{2\pi}cos(jx)sin(kx)dx &= 
\frac{1}{2} \int_0^{2 \pi} sin((j + k) x) + sin ((j - k )x) \\
\frac{1}{2} [-\frac{cos((j+k)x)}{j+k} - \frac{cos((j-k)x)}{j-k}] \vert_0^{2\pi}
\end{align*}

We note that $cos(k (2 \pi)) = 1 \forall k \in \mathbb{N}$ and $cos(0) = 1$. Therefore, for any natural 
numbers $j$ and $k$, $\frac{1}{2} [-\frac{cos((j+k)x)}{j+k} - \frac{cos((j-k)x)}{j-k}] \vert_0^{2\pi}= -1 - (-1)= 0$.

We should note here that 
\[
\int_0^{2\pi}sin(kx)cos(kx)dx
\]
would result in a divide-by-zero issue above. For this, we note that 
$\int_0^{2\pi}sin(kx)cos(kx)dx = -\frac{cos^2(kx)}{2k} \vert_0^{2\pi} = -\frac{1}{2k} - (- \frac{1}{2k}) =0$.

Therefore, we have show the Fourier Trigonometric Family to be orthogonal on $0 \leq x \leq 2\pi$ with respect to weight $w(x)=1$.

# Problem 2
## a) 
To show that if ${q_i(x)}$ is a set of orthogonal functions, then it is a linearly independent set, we use a proof by contradiction.
First, note that a set of orthogonal functions can trivialy be normalized, so we can assume without loss of generality that 
it is an orthonormal set.

Suppose it is not a linearly independent set. Then there must exist a function $q_k(x)$ such that $q_k(x) = \sum_{i=1}^{k-1} c_i q_i(x)$, 
where $\exists i \in [1, k-1]$ such that $c_i \neq 0$. 
(Note that we assume here that it is the last function in the set, $q_k$ out of notational convenience, but it doesn't really matter.)

Next, note that we can rewrite the sequence of constants, $c_i$ as 
\[
c_i = <q_k(x), q_i(x)>,  \forall 1 \leq i \leq k-1.
\]

Because the set of functions is orthogonal, we know that $<q_k(x), q_i(x)>=0$ for all $i \in [1, k-1]$. However, 
this is a contradiction, because $\nexists i \in [1, k-1]$ such that $c_i \neq 0$.

Therefore the set must be linearly independent, and we have show that a set of othogonal functions must be linearly independent too.

## b)
We first define $\rm{IMSE} = \int_D \rm{E}[(\hat{f}(x) - f(x))^2]$. 
Also, the expected value of $f(x)$ is a constant with respect to $\hat{f}(x)$.

Noting that the variance of $\hat{f}(x)$ can be written as $\rm{V}(\hat{f}) = \rm{E}[\hat{f}(x)^2] - \rm{E}[\hat{f}(x)]^2$, we rewrite $\rm{IMSE}$:
\begin{align*}
\rm{IMSE} &= \int_D \rm{E}[(\hat{f}(x) - f(x))^2] \\
&=\int_D \rm{E}[(\hat{f}(x)^2] - 2\rm{E}[(\hat{f}(x)f(x)] +  \rm{E}[f(x)^2] \\
&=\int_D \rm{V}(\hat{f})- 2\rm{E}[(\hat{f}(x)]f(x) +  f(x)^2 + \rm{E}[\hat{f})]^2 \\
&=\int_D \rm{V}(\hat{f}) + (\rm{E}[\hat{f}(x)] - f(x))^2 \\
&= \rm{IV}(\hat{f}) + \rm{ISB}(\hat{f}).
\end{align*}

# Problem 3
First, $\vert\vert\sum_{i=1}^m q_i\vert\vert^2$ can be expanded to be equal to $\sum_{i=1}^m \vert\vert q_i\vert\vert^2 + \sum_{j \neq k} \vert\vert q_j q_k \vert\vert$. Because the set of functions are orthogonal, 
$\sum_{j \neq k} \vert\vert q_j q_k=0 \vert\vert$. Therefore, 
$\vert\vert\sum_{i=1}^m q_i\vert\vert^2 = \sum_{i=1}^m \vert\vert q_i^2\vert\vert$. Under the L2 norm, 
$\sum_{i=1}^m \vert\vert q_i^2\vert\vert = \sum_{i=1}^m \vert\vert q_i\vert\vert^2$.

If each of the $q_k$ are orthonormal, the value of the expression will always evaluate to $m$, as it
will be the sum of $m$ functions equal to 1 over the space.

This expression may not hold under other norms, as the last step above, 
$\sum_{i=1}^m \vert\vert q_i^2\vert\vert = \sum_{i=1}^m \vert\vert q_i\vert\vert^2$ does not
necessarily hold under norms that are not L2, as the squared step is not transitive under the norm.

# Problem 4
## a)
We derive the first 4 Chebyshev Polynomials using the following formula:
\[
q_i(x) = x^i - \sum_{j=1}^{i-1} \frac{<q_i, q_j>}{<q_j(x), q_j(x)>}.
\]

(Shoutout to Wolfram for assisting with the integrals here.)

Noting that $\int_{-1}^{1} x^k \sqrt{1-x^2}dx = 0$ whenever $k$ is odd, and 
that $\int_{-1}^{1} x^2 \sqrt{1-x^2}dx = \frac{\pi}{8}$ and  $\int_{-1}^{1} x^4 \sqrt{1-x^2}dx = \frac{\pi}{16}$, we
can easily define our first four orthogonal polynomials:
\begin{align*}
q_0 &= 1 \\
q_1 &= x - 0 = x \\
q_2 &= x^2 - \frac{\frac{\pi}{8}}{\frac{\pi}{2}} - 0= x^2 - \frac{1}{4} \\
q_3 &= x^3 - 0- \frac{\frac{\pi}{16}}{\frac{\pi}{2}} - 0 = x^3 - \frac{1}{2}.
\end{align*}

Next, we normalize the polynomials via their inner product with themselves.
\begin{align*}
q_0 &= 2 / pi \\
q_1 &= \frac{8 x}{\pi} \\
q_2 &= \frac{32 (x^2 - 1/4)}{\pi} \\
q_3 &= \frac{128 (x^3 - 1/2)}{21 \pi}.
\end{align*}

## b)
Next, we take the density function described, $f(x) = \frac{1}{\sqrt{.6}} \rm{e}^{- \frac{x^2}{1.2}}$, and find the inner product 
of this function with each of the orthonormal functions. We use Wolfram to find the constants below:
\begin{align*}
c_0 &= 1.0696 \\
c_1 &= 0.0 \\ 
c_2 &= -0.799686 \\
c_3 &= -1.62987
\end{align*}


Next, we read in the data and see if we can approximate the the normal distribution with mean 0 and standard deviation .3.
```{r}
data <- scan("Orthogonal.txt")
approx <- function(x)   -1 * (1.62 * (128 * (x^3 - 1/2)) / (21 * pi)  + .8 * (32 * (x^2 - 1/4)) / (pi) + 1.0696 )
X <- seq(-1, 1, .01)
hist(data, freq=F)
lines(X, approx(X))
```

The density is not well approximated by the function. We likely need more than just the 4 terms to get a good approximation.


# Problem 5
## a)
The cubic spline described is a natural cubic spline, as $f^{\prime \prime}(-1) = f^{\prime \prime}(1) = 0$. However, it is not defined beyond
the endpoints, and so does not fulfill the requirement that the spline have a zero seond derivative beyond the end knots. The
spline is continuous throughout, as is a natural spline. The first derivative is not continuous at the middle knot - not a characteristic of a natrual spline. 

## b)
We follow the algorithm to calculate the cubic splines, first finding the following quantities.

The differences between points:
$w_i = x_{i+1} - x_i \implies (1, 1, 1)$

The first derivatives:
$h_i = \frac{y_{i+1} - y_i}{x_{i+1} - x_i} \implies (-\frac{1}{2}, -\frac{1}{6}, -\frac{1}{12})$

Enforcing naturality:
$f^{\prime\prime}_1 = 0$ and $f^{\prime\prime}_4 = 0$.

The second derivatives:
$f^{\prime\prime} = 3 \frac{h_{i+1} - h_i}{w_{i+1} + w_i} \implies (\frac{1}{12}, \frac{3}{8})$

For each knot, we then find 
$A_i = \frac{f^{\prime \prime}_{i+1} - f^{\prime\prime}_i}{6w_i}$, 
$B_i = \frac{f^{\prime\prime}_i}{2}$, 
$C_i = h_i - w_i\frac{f^{\prime\prime}_{i+1} + 2 f_{i}^{\prime\prime}}{6}$ and
$D_i = y_i$. We use R to help with the computation

```{r}
w_i <- rep(1, 3)
h_i <- c(-.5, -1/3, -1/12)
f2prime <- c(0, 1/4, 3/8, 0)

A <- (f2prime[2:4] - f2prime[1:3]) / 6
B <- f2prime / 2
C <- h_i - w_i * (f2prime[2:4] - 2 * f2prime[1:3]) / 6
D <- c(1, 1/2, 1/3, 1/4)
```
We write the entire spline as 
\[
s(x) = 
\begin{cases}
1 & x \leq 1 \\
\frac{1}{24} x^3 - \frac{13}{24}x + 1 & 1 < x \leq 2 \\
\frac{1}{48} x^3+ \frac{1}{8} x^2 - \frac{5}{16}x + \frac{1}{2} & 2 < x \leq 3 \\
-\frac{1}{16}x^3 + \frac{3}{16}x^2 - \frac{1}{24}x + \frac{1}{3} & 3 < x \leq  4 \\
0 & x > 4
\end{cases}
\].






















